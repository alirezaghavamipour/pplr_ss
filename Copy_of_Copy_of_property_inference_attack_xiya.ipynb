{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of property_inference_attack_xiya.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezaghavamipour/pplr_ss/blob/main/Copy_of_Copy_of_property_inference_attack_xiya.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9tlT1373NP-"
      },
      "source": [
        "## Possible Reason\n",
        "\n",
        "\n",
        "0.   overfitting --> 10-cross-validation is already perfomed during training\n",
        "1.   the size of meta-training set is relatively small --> **solution**: train more shadow models\n",
        "2.   the structure of classifier --> **solution**: compare two classifier\n",
        "3.   feature extraction --> **solution**: verify data and label\n",
        "4.   shadow models --> **solution**: force test accuracy above 0.9\n",
        "5.   training data --> **solution**: generate noisy training data/ mix it with original data\n",
        "6.   the similarity between training data --> **solution**: random sample training data from MNIST dataset\n",
        "7.   the shadow model is a linear model which is more easily to get similar trained model --> **solution**(***TODO***): visualize the meta-training set, PCA \n",
        "8.   too much parameters in baseline/neural sorting classifier --> **solution**(***TODO***): switch to other dataset to verify\n",
        "9.   the author made use of pytorch while I use tensorflow\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUGzd9xrU6E4"
      },
      "source": [
        "## Step 1  get training data for shadow model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHXFu-YPOxA7",
        "outputId": "fc668fd4-f434-416c-9a99-ccd809fd5174"
      },
      "source": [
        "!pip install keras\n",
        "!pip install kaggle\n",
        "\n",
        "# download NIST dataset for training shadow model\n",
        "!gdown --id 1w0EKrdoY01JuMDvH9wDD-ZUi23PZonrj\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "import os\n",
        "if not os.path.exists(\"./Data\"):\n",
        "    os.mkdir(\"./Data\")\n",
        "os.chdir(\"./Data\")\n",
        "!kaggle datasets download -d sachinpatel21/az-handwritten-alphabets-in-csv-format\n",
        "!unzip az-handwritten-alphabets-in-csv-format\n",
        "os.chdir(\"/content/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1w0EKrdoY01JuMDvH9wDD-ZUi23PZonrj\n",
            "To: /content/kaggle.json\n",
            "100% 64.0/64.0 [00:00<00:00, 134kB/s]\n",
            "Downloading az-handwritten-alphabets-in-csv-format.zip to /content/Data\n",
            " 94% 173M/185M [00:01<00:00, 132MB/s]\n",
            "100% 185M/185M [00:01<00:00, 131MB/s]\n",
            "Archive:  az-handwritten-alphabets-in-csv-format.zip\n",
            "  inflating: A_Z Handwritten Data.csv  \n",
            "  inflating: A_Z Handwritten Data/A_Z Handwritten Data.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH4pAE3Vd8qs"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pandas import read_csv\n",
        " \n",
        "def load_mnist_data():\n",
        "    mnist = tf.keras.datasets.mnist\n",
        "    (train_data, train_label), (test_data, test_label)  = mnist.load_data()              #Load data from folder\n",
        "    train_data, test_data = train_data.reshape(-1,28,28,1).astype('float32')/255.0, test_data.reshape(-1,28,28,1).astype('float32')/255.0\n",
        "    train_label, test_label = to_categorical(train_label), to_categorical(test_label)\n",
        " \n",
        "    return train_data, test_data, train_label, test_label\n",
        " \n",
        "def load_shadow_data():\n",
        "    data_path = \"Data/handwritten_charactor.npz\"\n",
        "    if os.path.exists(data_path):\n",
        "        with np.load(data_path) as f:\n",
        "            train_data, test_data, train_label, test_label = f['train_data'], f['test_data'], f['train_label'], f['test_label']\n",
        "    else:\n",
        "        file_path = \"Data/A_Z Handwritten Data.csv\"\n",
        "        data = read_csv(file_path)\n",
        "        label = to_categorical(data['0'].values)\n",
        "        data = data.drop(labels='0', axis='columns')\n",
        "        data = data.values.reshape(-1, 28, 28,1).astype('float32')/255.0\n",
        " \n",
        "        # random sample from data to create test data\n",
        "        index = np.sort(np.random.choice(range(data.shape[0]), int(data.shape[0]*0.2), replace=False))\n",
        " \n",
        "        test_data = data[index]\n",
        "        train_data = np.delete(data, index, axis=0)\n",
        "        test_label = label[index]\n",
        "        train_label = np.delete(label, index, axis=0)\n",
        " \n",
        "        with open(data_path, 'wb') as f:\n",
        "            np.savez_compressed(f, train_data = train_data, test_data = test_data, train_label = train_label, test_label = test_label)\n",
        " \n",
        "    return train_data, test_data, train_label, test_label  \n",
        " \n",
        "def load_noisy_data(is_shadow, brightness_range=(0,10), train_data = None, test_data = None, train_label = None, test_label = None):\n",
        "    if is_shadow:\n",
        "        file_path = \"./Data/shadow_noisy_{}_{}.npz\".format(brightness_range[0], brightness_range[1])\n",
        "    else:\n",
        "        file_path = \"./Data/mnist_noisy_{}_{}.npz\".format(brightness_range[0], brightness_range[1])\n",
        "    if os.path.exists(file_path):\n",
        "        with np.load(file_path) as loaded:\n",
        "            train_data_noisy = loaded['train_data_noisy']\n",
        "            train_label_noisy = loaded['train_label_noisy']\n",
        "            test_data_noisy = loaded['test_data_noisy']\n",
        "            test_label_noisy = loaded['test_label_noisy']\n",
        "    else:\n",
        "        # create the noisy data generator\n",
        "        datagen = ImageDataGenerator(\n",
        "            brightness_range = brightness_range\n",
        "        )\n",
        " \n",
        "        train_data_noisy_generator = datagen.flow(train_data, train_label, batch_size=1, shuffle=False)\n",
        "        test_data_noisy_generator = datagen.flow(test_data, test_label, batch_size=1, shuffle=False)\n",
        " \n",
        "        train_data_noisy =  np.empty(train_data.shape)\n",
        "        train_label_noisy =  np.empty(train_label.shape)\n",
        "        test_data_noisy = np.empty(test_data.shape)\n",
        "        test_label_noisy = np.empty(test_label.shape)\n",
        " \n",
        "        # iterate though all train_data to geneate the noisy data\n",
        "        for  i in range(len(train_data)):\n",
        "            train_data_noisy[i], train_label_noisy[i] = next(train_data_noisy_generator)\n",
        " \n",
        "        for  i in range(len(test_data)):\n",
        "            test_data_noisy[i], test_label_noisy[i] = next(test_data_noisy_generator)\n",
        " \n",
        "        train_data_noisy, test_data_noisy = train_data_noisy/255, test_data_noisy/255\n",
        " \n",
        "        with open(file_path, 'wb') as f:\n",
        "            np.savez_compressed(f, train_data_noisy = train_data_noisy, test_data_noisy = test_data_noisy, train_label_noisy = train_label_noisy, test_label_noisy = test_label_noisy)\n",
        " \n",
        "    return train_data_noisy, test_data_noisy, train_label_noisy, test_label_noisy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4fyLQyniWla"
      },
      "source": [
        "## Step 2 create the shadow model \n",
        "Create the shadow model with three hidden fully connected layer with 128, 32, and 16 parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_n5tZb6PgiB"
      },
      "source": [
        "from pandas import  DataFrame\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        " \n",
        "def get_model(output=10):\n",
        "    model = Sequential(                      #define model structure\n",
        "            [\n",
        "                Input(shape=(28,28,1)),\n",
        "                Flatten(),\n",
        "                Dense(128, activation=\"relu\"),\n",
        "                Dense(32, activation=\"relu\"),\n",
        "                Dense(16, activation=\"relu\"),\n",
        "                Dense(output, activation=\"softmax\")\n",
        "            ]\n",
        "        )\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnxL64dci4dU"
      },
      "source": [
        "Funtion \"save_model\" for save parameters of shadow models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLqYoup_VXuX"
      },
      "source": [
        "def save_model(model, history, modelname):\n",
        "    if not os.path.exists(\"./Models/\"):\n",
        "      os.mkdir(\"./Models/\")\n",
        "    FILENAME = \"./Models/\" + modelname                #Models are stored in a folder\n",
        "    HISTNAME = FILENAME + '/history.csv'                  #The history file\n",
        "    model.save(FILENAME)                                                #save the model\n",
        "    print(\"Model saved at \" + FILENAME)\n",
        "    DataFrame.from_dict(history).to_csv(HISTNAME,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6pcjW3PlkuT"
      },
      "source": [
        "## Step 3 Training models\n",
        "Function \"train_model\" for training a pair of shadow model based on original data and noisy data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbSBFYMcVb2J"
      },
      "source": [
        "def train_model(name):\n",
        "    print(\"Start training model \" + str(name))\n",
        "    if is_shadow:\n",
        "        model_original, model_noisy = get_model(26), get_model(26)\n",
        "    else:\n",
        "        model_original, model_noisy = get_model(), get_model()\n",
        "    model_original.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    model_noisy.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    # train a model based on original data\n",
        "    test_accur = 0\n",
        "    while test_accur < 0.9:\n",
        "        sample_index = np.random.choice(range(0,len(train_image)), int(0.2*len(train_image)), replace=False)\n",
        "        history_original = model_original.fit(train_image[sample_index], train_label[sample_index], epochs = 5, validation_split = 0.1)\n",
        "        _, test_accur = model_original.evaluate(test_image, test_label)\n",
        "    save_model(model_original, history_original.history, PREFIX_ORIGINAL + str(name))\n",
        " \n",
        "    test_accur = 0\n",
        "    while test_accur < 0.9:\n",
        "        # train a model based on noisy data\n",
        "        sample_index = np.random.choice(range(0,len(train_image_noisy)), int(0.2*len(train_image_noisy)), replace=False)\n",
        "        history_noisy = model_noisy.fit(train_image_noisy[sample_index], train_label_noisy[sample_index], epochs = 5, validation_split = 0.1)\n",
        "        _, test_accur = model_noisy.evaluate(test_image_noisy, test_label_noisy)\n",
        "    save_model(model_noisy, history_noisy.history, PREFIX_NOISY + str(name))\n",
        " \n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWxtcze2mynd"
      },
      "source": [
        "Batch train models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGwcCHMbciwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b7e2157-848c-4033-97df-e17a8db312f7"
      },
      "source": [
        "# Prepare data\n",
        " \n",
        "dataset = \"mnist\"\n",
        "is_shadow = int(input(\"Please supply model type(0: target model, 1: shadow model): \"))\n",
        "if  is_shadow:\n",
        "    train_image, test_image, train_label, test_label = load_shadow_data()\n",
        "else:\n",
        "    train_image, test_image, train_label, test_label = load_mnist_data()\n",
        "brightness_range = (0,5)\n",
        "train_image_noisy, test_image_noisy, train_label_noisy, test_label_noisy = load_noisy_data(is_shadow,brightness_range, train_image, test_image, train_label, test_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please supply model type(0: target model, 1: shadow model): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "uZ829nnPrQ2v",
        "outputId": "80a11eb9-bb49-4585-dfaf-7ecc83aeeb78"
      },
      "source": [
        "def show_sample_noisy_image(train_image, train_image_noisy):\n",
        "    fig = plt.figure(figsize=(12,12))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.title(\"original image\")\n",
        "    plt.imshow(train_image.reshape(28,28),cmap='gray')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.title(\"noisy image [5,10]\")\n",
        "    plt.imshow(train_image_noisy.reshape(28,28),cmap='gray')\n",
        "\n",
        "# Show the comparison of original image and noisy data\n",
        "show_sample_noisy_image(train_image[0], train_image_noisy[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x864 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPqrR8j6bqY8",
        "outputId": "55feed49-d1df-48ca-9ef1-6f6e1f2548b5"
      },
      "source": [
        "# train models\n",
        "start_number = int(input(\"Supply the start number:\"))\n",
        "times = int(input(\"Supply the training times:\"))\n",
        "PREFIX = ['TARGET-', 'SHADOW-'][is_shadow]\n",
        "\n",
        "PREFIX_ORIGINAL = PREFIX + \"ORIGINAL-\"\n",
        "PREFIX_NOISY = PREFIX +  \"NOISY-\"\n",
        "if times and PREFIX:\n",
        "    for i in range(start_number, start_number + times):\n",
        "      train_model(i)\n",
        "else:\n",
        "    print(\"Training stopped.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supply the start number:0\n",
            "Supply the training times:20\n",
            "Start training model 0\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.7158 - accuracy: 0.8070 - val_loss: 0.3840 - val_accuracy: 0.8955\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2898 - accuracy: 0.9186 - val_loss: 0.2878 - val_accuracy: 0.9228\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2110 - accuracy: 0.9412 - val_loss: 0.2520 - val_accuracy: 0.9307\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.1686 - accuracy: 0.9523 - val_loss: 0.2241 - val_accuracy: 0.9426\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.1430 - accuracy: 0.9582 - val_loss: 0.2178 - val_accuracy: 0.9440\n",
            "2328/2328 [==============================] - 7s 3ms/step - loss: 0.1987 - accuracy: 0.9447\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-0/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-0\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.8488 - accuracy: 0.7800 - val_loss: 0.4367 - val_accuracy: 0.8904\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.3846 - accuracy: 0.9031 - val_loss: 0.3294 - val_accuracy: 0.9119\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2925 - accuracy: 0.9245 - val_loss: 0.3006 - val_accuracy: 0.9190\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2401 - accuracy: 0.9370 - val_loss: 0.2714 - val_accuracy: 0.9307\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2052 - accuracy: 0.9442 - val_loss: 0.2596 - val_accuracy: 0.9351\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2772 - accuracy: 0.9313\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-0/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-0\n",
            "Start training model 1\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.7310 - accuracy: 0.7981 - val_loss: 0.3882 - val_accuracy: 0.9007\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2879 - accuracy: 0.9191 - val_loss: 0.2954 - val_accuracy: 0.9161\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2135 - accuracy: 0.9394 - val_loss: 0.2360 - val_accuracy: 0.9344\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.1734 - accuracy: 0.9505 - val_loss: 0.2222 - val_accuracy: 0.9411\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.1455 - accuracy: 0.9583 - val_loss: 0.2174 - val_accuracy: 0.9409\n",
            "2328/2328 [==============================] - 7s 3ms/step - loss: 0.2058 - accuracy: 0.9450\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-1/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-1\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.8887 - accuracy: 0.7733 - val_loss: 0.4807 - val_accuracy: 0.8748\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.3970 - accuracy: 0.8973 - val_loss: 0.3701 - val_accuracy: 0.9057\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 5s 3ms/step - loss: 0.2996 - accuracy: 0.9208 - val_loss: 0.3262 - val_accuracy: 0.9185\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2432 - accuracy: 0.9351 - val_loss: 0.3090 - val_accuracy: 0.9176\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2089 - accuracy: 0.9438 - val_loss: 0.3006 - val_accuracy: 0.9223\n",
            "2328/2328 [==============================] - 7s 3ms/step - loss: 0.2949 - accuracy: 0.9268\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-1/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-1\n",
            "Start training model 2\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.7200 - accuracy: 0.8036 - val_loss: 0.3570 - val_accuracy: 0.9013\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2848 - accuracy: 0.9207 - val_loss: 0.2424 - val_accuracy: 0.9300\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2087 - accuracy: 0.9414 - val_loss: 0.1987 - val_accuracy: 0.9398\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.1655 - accuracy: 0.9529 - val_loss: 0.1900 - val_accuracy: 0.9445\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.1398 - accuracy: 0.9594 - val_loss: 0.1780 - val_accuracy: 0.9500\n",
            "2328/2328 [==============================] - 7s 3ms/step - loss: 0.1990 - accuracy: 0.9450\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-2/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-2\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.8665 - accuracy: 0.7791 - val_loss: 0.4685 - val_accuracy: 0.8856\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.4090 - accuracy: 0.8982 - val_loss: 0.3527 - val_accuracy: 0.9156\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.3118 - accuracy: 0.9209 - val_loss: 0.3103 - val_accuracy: 0.9227\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2582 - accuracy: 0.9341 - val_loss: 0.2993 - val_accuracy: 0.9238\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2203 - accuracy: 0.9417 - val_loss: 0.2630 - val_accuracy: 0.9374\n",
            "2328/2328 [==============================] - 7s 3ms/step - loss: 0.2875 - accuracy: 0.9323\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-2/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-2\n",
            "Start training model 3\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.7430 - accuracy: 0.7917 - val_loss: 0.3531 - val_accuracy: 0.9007\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2829 - accuracy: 0.9206 - val_loss: 0.2341 - val_accuracy: 0.9331\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2051 - accuracy: 0.9429 - val_loss: 0.2177 - val_accuracy: 0.9369\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.1629 - accuracy: 0.9525 - val_loss: 0.2046 - val_accuracy: 0.9408\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1396 - accuracy: 0.9592 - val_loss: 0.2052 - val_accuracy: 0.9413\n",
            "2328/2328 [==============================] - 7s 3ms/step - loss: 0.1995 - accuracy: 0.9471\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-3/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-3\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.8204 - accuracy: 0.7872 - val_loss: 0.4668 - val_accuracy: 0.8827\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3868 - accuracy: 0.9018 - val_loss: 0.3374 - val_accuracy: 0.9159\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2974 - accuracy: 0.9214 - val_loss: 0.3102 - val_accuracy: 0.9223\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2419 - accuracy: 0.9357 - val_loss: 0.2871 - val_accuracy: 0.9275\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2103 - accuracy: 0.9429 - val_loss: 0.2855 - val_accuracy: 0.9287\n",
            "2328/2328 [==============================] - 7s 3ms/step - loss: 0.2800 - accuracy: 0.9315\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-3/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-3\n",
            "Start training model 4\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.7698 - accuracy: 0.7905 - val_loss: 0.3702 - val_accuracy: 0.8971\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2995 - accuracy: 0.9169 - val_loss: 0.2683 - val_accuracy: 0.9237\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2174 - accuracy: 0.9388 - val_loss: 0.2345 - val_accuracy: 0.9352\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.1765 - accuracy: 0.9486 - val_loss: 0.2045 - val_accuracy: 0.9404\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.1484 - accuracy: 0.9573 - val_loss: 0.1985 - val_accuracy: 0.9399\n",
            "2328/2328 [==============================] - 7s 3ms/step - loss: 0.2160 - accuracy: 0.9422\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-4/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-4\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.8315 - accuracy: 0.7887 - val_loss: 0.4958 - val_accuracy: 0.8747\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.3858 - accuracy: 0.9016 - val_loss: 0.3801 - val_accuracy: 0.9015\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2951 - accuracy: 0.9216 - val_loss: 0.3261 - val_accuracy: 0.9166\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2425 - accuracy: 0.9360 - val_loss: 0.2837 - val_accuracy: 0.9279\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2051 - accuracy: 0.9441 - val_loss: 0.2896 - val_accuracy: 0.9262\n",
            "2328/2328 [==============================] - 7s 3ms/step - loss: 0.2845 - accuracy: 0.9312\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-4/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-4\n",
            "Start training model 5\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.7397 - accuracy: 0.8008 - val_loss: 0.3474 - val_accuracy: 0.9015\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2947 - accuracy: 0.9162 - val_loss: 0.2736 - val_accuracy: 0.9195\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2154 - accuracy: 0.9384 - val_loss: 0.2152 - val_accuracy: 0.9391\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.1720 - accuracy: 0.9512 - val_loss: 0.2014 - val_accuracy: 0.9418\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.1470 - accuracy: 0.9571 - val_loss: 0.1867 - val_accuracy: 0.9520\n",
            "2328/2328 [==============================] - 7s 3ms/step - loss: 0.2064 - accuracy: 0.9445\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-5/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-5\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.9027 - accuracy: 0.7720 - val_loss: 0.4592 - val_accuracy: 0.8864\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.3812 - accuracy: 0.9029 - val_loss: 0.3607 - val_accuracy: 0.9091\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2903 - accuracy: 0.9256 - val_loss: 0.3268 - val_accuracy: 0.9148\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2389 - accuracy: 0.9376 - val_loss: 0.2725 - val_accuracy: 0.9294\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2036 - accuracy: 0.9441 - val_loss: 0.2671 - val_accuracy: 0.9344\n",
            "2328/2328 [==============================] - 7s 3ms/step - loss: 0.2680 - accuracy: 0.9343\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-5/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-5\n",
            "Start training model 6\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.7293 - accuracy: 0.8002 - val_loss: 0.3607 - val_accuracy: 0.9025\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2976 - accuracy: 0.9176 - val_loss: 0.2925 - val_accuracy: 0.9188\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2180 - accuracy: 0.9395 - val_loss: 0.2454 - val_accuracy: 0.9292\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.1752 - accuracy: 0.9500 - val_loss: 0.2195 - val_accuracy: 0.9372\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1461 - accuracy: 0.9573 - val_loss: 0.1859 - val_accuracy: 0.9493\n",
            "2328/2328 [==============================] - 7s 3ms/step - loss: 0.1908 - accuracy: 0.9474\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-6/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-6\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.9029 - accuracy: 0.7646 - val_loss: 0.4854 - val_accuracy: 0.8807\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.4068 - accuracy: 0.8947 - val_loss: 0.4079 - val_accuracy: 0.9030\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3075 - accuracy: 0.9202 - val_loss: 0.3266 - val_accuracy: 0.9201\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2541 - accuracy: 0.9322 - val_loss: 0.3178 - val_accuracy: 0.9277\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2170 - accuracy: 0.9418 - val_loss: 0.3342 - val_accuracy: 0.9235\n",
            "2328/2328 [==============================] - 7s 3ms/step - loss: 0.3119 - accuracy: 0.9231\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-6/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-6\n",
            "Start training model 7\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.6905 - accuracy: 0.8131 - val_loss: 0.3512 - val_accuracy: 0.9025\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2844 - accuracy: 0.9198 - val_loss: 0.2712 - val_accuracy: 0.9272\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.2079 - accuracy: 0.9395 - val_loss: 0.2546 - val_accuracy: 0.9240\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1695 - accuracy: 0.9505 - val_loss: 0.2105 - val_accuracy: 0.9411\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 3ms/step - loss: 0.1426 - accuracy: 0.9583 - val_loss: 0.2005 - val_accuracy: 0.9458\n",
            "2328/2328 [==============================] - 7s 3ms/step - loss: 0.1929 - accuracy: 0.9461\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-7/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-7\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.8866 - accuracy: 0.7700 - val_loss: 0.4912 - val_accuracy: 0.8782\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3820 - accuracy: 0.9008 - val_loss: 0.3770 - val_accuracy: 0.9054\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2915 - accuracy: 0.9241 - val_loss: 0.3100 - val_accuracy: 0.9210\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2403 - accuracy: 0.9352 - val_loss: 0.2844 - val_accuracy: 0.9295\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2065 - accuracy: 0.9443 - val_loss: 0.2712 - val_accuracy: 0.9351\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2709 - accuracy: 0.9331\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-7/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-7\n",
            "Start training model 8\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.7536 - accuracy: 0.7951 - val_loss: 0.3691 - val_accuracy: 0.8982\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2868 - accuracy: 0.9195 - val_loss: 0.2639 - val_accuracy: 0.9280\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.2081 - accuracy: 0.9404 - val_loss: 0.2372 - val_accuracy: 0.9346\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1681 - accuracy: 0.9509 - val_loss: 0.2355 - val_accuracy: 0.9327\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1412 - accuracy: 0.9586 - val_loss: 0.2142 - val_accuracy: 0.9389\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2073 - accuracy: 0.9448\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-8/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-8\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.9498 - accuracy: 0.7525 - val_loss: 0.5172 - val_accuracy: 0.8721\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.4158 - accuracy: 0.8947 - val_loss: 0.3632 - val_accuracy: 0.9133\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3090 - accuracy: 0.9203 - val_loss: 0.3172 - val_accuracy: 0.9213\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2522 - accuracy: 0.9335 - val_loss: 0.3110 - val_accuracy: 0.9220\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2183 - accuracy: 0.9419 - val_loss: 0.2825 - val_accuracy: 0.9309\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2843 - accuracy: 0.9305\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-8/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-8\n",
            "Start training model 9\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.7372 - accuracy: 0.7978 - val_loss: 0.3611 - val_accuracy: 0.9005\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2954 - accuracy: 0.9161 - val_loss: 0.2999 - val_accuracy: 0.9193\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2185 - accuracy: 0.9378 - val_loss: 0.2379 - val_accuracy: 0.9357\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1757 - accuracy: 0.9489 - val_loss: 0.2096 - val_accuracy: 0.9435\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1479 - accuracy: 0.9573 - val_loss: 0.1975 - val_accuracy: 0.9485\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2047 - accuracy: 0.9451\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-9/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-9\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.8622 - accuracy: 0.7749 - val_loss: 0.4635 - val_accuracy: 0.8857\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3936 - accuracy: 0.8985 - val_loss: 0.3591 - val_accuracy: 0.9109\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3000 - accuracy: 0.9207 - val_loss: 0.2943 - val_accuracy: 0.9260\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2456 - accuracy: 0.9343 - val_loss: 0.2744 - val_accuracy: 0.9327\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2083 - accuracy: 0.9436 - val_loss: 0.2557 - val_accuracy: 0.9357\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2830 - accuracy: 0.9299\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-9/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-9\n",
            "Start training model 10\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.7661 - accuracy: 0.7923 - val_loss: 0.3513 - val_accuracy: 0.9010\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.2920 - accuracy: 0.9199 - val_loss: 0.2584 - val_accuracy: 0.9268\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2156 - accuracy: 0.9392 - val_loss: 0.2109 - val_accuracy: 0.9378\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1762 - accuracy: 0.9487 - val_loss: 0.1995 - val_accuracy: 0.9426\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1463 - accuracy: 0.9572 - val_loss: 0.1873 - val_accuracy: 0.9482\n",
            "2328/2328 [==============================] - 8s 4ms/step - loss: 0.1993 - accuracy: 0.9475\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-10/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-10\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.8361 - accuracy: 0.7865 - val_loss: 0.4097 - val_accuracy: 0.8928\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3809 - accuracy: 0.9030 - val_loss: 0.3595 - val_accuracy: 0.9069\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.2950 - accuracy: 0.9245 - val_loss: 0.3169 - val_accuracy: 0.9161\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2409 - accuracy: 0.9354 - val_loss: 0.2485 - val_accuracy: 0.9336\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2043 - accuracy: 0.9440 - val_loss: 0.2549 - val_accuracy: 0.9322\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2974 - accuracy: 0.9269\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-10/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-10\n",
            "Start training model 11\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.7596 - accuracy: 0.7893 - val_loss: 0.3795 - val_accuracy: 0.8973\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2936 - accuracy: 0.9177 - val_loss: 0.2731 - val_accuracy: 0.9267\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2173 - accuracy: 0.9390 - val_loss: 0.2337 - val_accuracy: 0.9352\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.1729 - accuracy: 0.9509 - val_loss: 0.2267 - val_accuracy: 0.9329\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1446 - accuracy: 0.9574 - val_loss: 0.2065 - val_accuracy: 0.9411\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2109 - accuracy: 0.9421\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-11/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-11\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.8783 - accuracy: 0.7749 - val_loss: 0.4768 - val_accuracy: 0.8814\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3959 - accuracy: 0.8994 - val_loss: 0.3623 - val_accuracy: 0.9101\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2958 - accuracy: 0.9234 - val_loss: 0.3023 - val_accuracy: 0.9193\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.2434 - accuracy: 0.9362 - val_loss: 0.2769 - val_accuracy: 0.9304\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.2091 - accuracy: 0.9442 - val_loss: 0.2641 - val_accuracy: 0.9341\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2791 - accuracy: 0.9309\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-11/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-11\n",
            "Start training model 12\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.7340 - accuracy: 0.7966 - val_loss: 0.3633 - val_accuracy: 0.8988\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.2917 - accuracy: 0.9185 - val_loss: 0.2688 - val_accuracy: 0.9262\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.2115 - accuracy: 0.9402 - val_loss: 0.2239 - val_accuracy: 0.9426\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.1722 - accuracy: 0.9504 - val_loss: 0.2095 - val_accuracy: 0.9435\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.1443 - accuracy: 0.9578 - val_loss: 0.2153 - val_accuracy: 0.9430\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2112 - accuracy: 0.9423\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-12/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-12\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.8727 - accuracy: 0.7748 - val_loss: 0.4933 - val_accuracy: 0.8775\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3950 - accuracy: 0.9005 - val_loss: 0.3980 - val_accuracy: 0.8985\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3048 - accuracy: 0.9205 - val_loss: 0.3420 - val_accuracy: 0.9159\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2538 - accuracy: 0.9333 - val_loss: 0.3124 - val_accuracy: 0.9218\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2144 - accuracy: 0.9431 - val_loss: 0.3010 - val_accuracy: 0.9285\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2852 - accuracy: 0.9283\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-12/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-12\n",
            "Start training model 13\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.7002 - accuracy: 0.8086 - val_loss: 0.3684 - val_accuracy: 0.9032\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2853 - accuracy: 0.9211 - val_loss: 0.2771 - val_accuracy: 0.9196\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2116 - accuracy: 0.9410 - val_loss: 0.2292 - val_accuracy: 0.9386\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1689 - accuracy: 0.9517 - val_loss: 0.2059 - val_accuracy: 0.9443\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1448 - accuracy: 0.9576 - val_loss: 0.2097 - val_accuracy: 0.9441\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2111 - accuracy: 0.9441\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-13/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-13\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.8513 - accuracy: 0.7804 - val_loss: 0.4987 - val_accuracy: 0.8748\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3893 - accuracy: 0.9003 - val_loss: 0.3461 - val_accuracy: 0.9141\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2945 - accuracy: 0.9241 - val_loss: 0.3255 - val_accuracy: 0.9174\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2440 - accuracy: 0.9365 - val_loss: 0.3103 - val_accuracy: 0.9213\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2085 - accuracy: 0.9435 - val_loss: 0.2859 - val_accuracy: 0.9300\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2687 - accuracy: 0.9336\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-13/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-13\n",
            "Start training model 14\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.7929 - accuracy: 0.7839 - val_loss: 0.3649 - val_accuracy: 0.8965\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3042 - accuracy: 0.9151 - val_loss: 0.2789 - val_accuracy: 0.9223\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2206 - accuracy: 0.9382 - val_loss: 0.2193 - val_accuracy: 0.9388\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1747 - accuracy: 0.9506 - val_loss: 0.2027 - val_accuracy: 0.9421\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1468 - accuracy: 0.9577 - val_loss: 0.1855 - val_accuracy: 0.9487\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2010 - accuracy: 0.9458\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-14/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-14\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.8568 - accuracy: 0.7763 - val_loss: 0.4513 - val_accuracy: 0.8864\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3937 - accuracy: 0.8994 - val_loss: 0.3544 - val_accuracy: 0.9112\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2980 - accuracy: 0.9239 - val_loss: 0.3091 - val_accuracy: 0.9206\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2489 - accuracy: 0.9355 - val_loss: 0.3063 - val_accuracy: 0.9240\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2094 - accuracy: 0.9434 - val_loss: 0.2866 - val_accuracy: 0.9282\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2886 - accuracy: 0.9285\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-14/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-14\n",
            "Start training model 15\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.7062 - accuracy: 0.8068 - val_loss: 0.4186 - val_accuracy: 0.8832\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2816 - accuracy: 0.9201 - val_loss: 0.2715 - val_accuracy: 0.9227\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2037 - accuracy: 0.9425 - val_loss: 0.2612 - val_accuracy: 0.9263\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1624 - accuracy: 0.9530 - val_loss: 0.2275 - val_accuracy: 0.9378\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1352 - accuracy: 0.9606 - val_loss: 0.2179 - val_accuracy: 0.9398\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2064 - accuracy: 0.9441\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-15/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-15\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.8907 - accuracy: 0.7709 - val_loss: 0.4987 - val_accuracy: 0.8770\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.4173 - accuracy: 0.8952 - val_loss: 0.3794 - val_accuracy: 0.9086\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3127 - accuracy: 0.9186 - val_loss: 0.3475 - val_accuracy: 0.9117\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2619 - accuracy: 0.9320 - val_loss: 0.3207 - val_accuracy: 0.9186\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2214 - accuracy: 0.9409 - val_loss: 0.2835 - val_accuracy: 0.9297\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2720 - accuracy: 0.9314\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-15/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-15\n",
            "Start training model 16\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.7457 - accuracy: 0.7916 - val_loss: 0.3768 - val_accuracy: 0.8966\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2965 - accuracy: 0.9184 - val_loss: 0.2712 - val_accuracy: 0.9237\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2174 - accuracy: 0.9394 - val_loss: 0.2411 - val_accuracy: 0.9324\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1742 - accuracy: 0.9498 - val_loss: 0.2223 - val_accuracy: 0.9357\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1457 - accuracy: 0.9582 - val_loss: 0.2036 - val_accuracy: 0.9430\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.1924 - accuracy: 0.9475\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-16/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-16\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.8415 - accuracy: 0.7818 - val_loss: 0.4675 - val_accuracy: 0.8834\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3785 - accuracy: 0.9032 - val_loss: 0.3573 - val_accuracy: 0.9094\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2897 - accuracy: 0.9249 - val_loss: 0.3322 - val_accuracy: 0.9121\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2438 - accuracy: 0.9349 - val_loss: 0.3304 - val_accuracy: 0.9141\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2078 - accuracy: 0.9425 - val_loss: 0.2686 - val_accuracy: 0.9300\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2640 - accuracy: 0.9323\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-16/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-16\n",
            "Start training model 17\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.7052 - accuracy: 0.8043 - val_loss: 0.3400 - val_accuracy: 0.9079\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2777 - accuracy: 0.9217 - val_loss: 0.2810 - val_accuracy: 0.9198\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2025 - accuracy: 0.9427 - val_loss: 0.2325 - val_accuracy: 0.9322\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1649 - accuracy: 0.9529 - val_loss: 0.2563 - val_accuracy: 0.9285\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1407 - accuracy: 0.9584 - val_loss: 0.1955 - val_accuracy: 0.9477\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.1872 - accuracy: 0.9495\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-17/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-17\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.8497 - accuracy: 0.7826 - val_loss: 0.4920 - val_accuracy: 0.8773\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3796 - accuracy: 0.9033 - val_loss: 0.3755 - val_accuracy: 0.9076\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2894 - accuracy: 0.9255 - val_loss: 0.3463 - val_accuracy: 0.9148\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2394 - accuracy: 0.9363 - val_loss: 0.2904 - val_accuracy: 0.9280\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2054 - accuracy: 0.9441 - val_loss: 0.3184 - val_accuracy: 0.9237\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2888 - accuracy: 0.9280\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-17/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-17\n",
            "Start training model 18\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.7073 - accuracy: 0.8046 - val_loss: 0.3347 - val_accuracy: 0.9086\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2745 - accuracy: 0.9229 - val_loss: 0.2529 - val_accuracy: 0.9280\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2042 - accuracy: 0.9413 - val_loss: 0.2201 - val_accuracy: 0.9413\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1680 - accuracy: 0.9511 - val_loss: 0.2012 - val_accuracy: 0.9441\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1381 - accuracy: 0.9593 - val_loss: 0.2227 - val_accuracy: 0.9391\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2082 - accuracy: 0.9418\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-18/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-18\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.8699 - accuracy: 0.7767 - val_loss: 0.4703 - val_accuracy: 0.8839\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3763 - accuracy: 0.9029 - val_loss: 0.3744 - val_accuracy: 0.9087\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2831 - accuracy: 0.9263 - val_loss: 0.3275 - val_accuracy: 0.9183\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2350 - accuracy: 0.9380 - val_loss: 0.3031 - val_accuracy: 0.9262\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.1974 - accuracy: 0.9476 - val_loss: 0.2661 - val_accuracy: 0.9362\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2672 - accuracy: 0.9348\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-18/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-18\n",
            "Start training model 19\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.7510 - accuracy: 0.7923 - val_loss: 0.3371 - val_accuracy: 0.9077\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2852 - accuracy: 0.9196 - val_loss: 0.2591 - val_accuracy: 0.9280\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2083 - accuracy: 0.9401 - val_loss: 0.2152 - val_accuracy: 0.9419\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1667 - accuracy: 0.9517 - val_loss: 0.2098 - val_accuracy: 0.9398\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1415 - accuracy: 0.9585 - val_loss: 0.1900 - val_accuracy: 0.9490\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.1923 - accuracy: 0.9476\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-ORIGINAL-19/assets\n",
            "Model saved at ./Models/SHADOW-ORIGINAL-19\n",
            "Epoch 1/5\n",
            "1676/1676 [==============================] - 7s 4ms/step - loss: 0.8962 - accuracy: 0.7680 - val_loss: 0.4884 - val_accuracy: 0.8753\n",
            "Epoch 2/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.3802 - accuracy: 0.9032 - val_loss: 0.3900 - val_accuracy: 0.8997\n",
            "Epoch 3/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2852 - accuracy: 0.9258 - val_loss: 0.3354 - val_accuracy: 0.9136\n",
            "Epoch 4/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.2347 - accuracy: 0.9386 - val_loss: 0.3098 - val_accuracy: 0.9190\n",
            "Epoch 5/5\n",
            "1676/1676 [==============================] - 6s 4ms/step - loss: 0.1979 - accuracy: 0.9462 - val_loss: 0.2917 - val_accuracy: 0.9257\n",
            "2328/2328 [==============================] - 8s 3ms/step - loss: 0.2814 - accuracy: 0.9317\n",
            "INFO:tensorflow:Assets written to: ./Models/SHADOW-NOISY-19/assets\n",
            "Model saved at ./Models/SHADOW-NOISY-19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2nsSeVynL6a"
      },
      "source": [
        "## Step 4 Feature extraction\n",
        "In this section, we will extract features from saved models.\n",
        "\n",
        "1.   Baseline approach: function ***load_variables_baseline***,  \n",
        "\n",
        "  for each layer:\n",
        "  * load weights from all nodes, \n",
        "  * flatten it to a 1d vector. \n",
        "2.   Neural sorting approach:  function ***load_variables_soritng***, \n",
        "\n",
        "  for each layer:\n",
        "  * load weights from each node, \n",
        "  * calculate the sum of weights,\n",
        "  * sort nodes by the value of sum. \n",
        "\n",
        "  concanenage the obtained node vectors from three layer and flatten it to a 1d vector.\n",
        "3.   Deepset: *load_variables_deepset*: ***load_variables_deepset***, \n",
        "\n",
        "  for each layer:\n",
        " * load weights from each node, \n",
        " * concananent each weight matrix with bias, \n",
        " * flatten each matrix to a 1d vector as node representation.\n",
        " \n",
        " return an list consists of three arrays of node presentations from each layer.\n",
        "\n",
        "The functions with suffix ***from_model***  are used for loading variables from a single model, while functions without the suffix are used for batch loading variables from a set of models.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mue02AJLngKQ"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from multiprocessing import Pool\n",
        "from os import path\n",
        "from pandas import DataFrame\n",
        "\n",
        "from_shadow = True\n",
        "PREFIX = 'TARGET-'\n",
        "prefix_prop = 'NOISY-'\n",
        "model_folder = './Models/'\n",
        "approach = 0\n",
        "\n",
        "def extract_features_from_model(index):\n",
        "    prefix = [PREFIX + \"ORIGINAL-\", PREFIX + prefix_prop]\n",
        "    vectors = []\n",
        "    labels = []\n",
        "    for p in prefix:\n",
        "        vector = []\n",
        "        model_path = model_folder + p + str(index)\n",
        "        if path.exists(model_path):\n",
        "            model = load_model(model_path)\n",
        "            # ignore the flatten layer and the output layer\n",
        "            for layer in model.layers[1:-1]:\n",
        "                if approach == 0:\n",
        "                    weight = layer.weights[0].numpy().flatten()\n",
        "                    weight = np.concatenate((weight, layer.weights[1].numpy().flatten()))\n",
        "                    vector = weight if len(vector) == 0 else np.concatenate((vector, weight))\n",
        "                elif approach == 1:\n",
        "                    weight_sum = np.sum(layer.trainable_weights[0].numpy().T, axis=1)\n",
        "                    weight_sum = np.argsort(weight_sum)                    \n",
        "                    weight = np.column_stack((layer.trainable_weights[0].numpy().T, layer.trainable_weights[1].numpy().T))\n",
        "                    weight = weight[weight_sum]\n",
        "                    vector = weight.flatten() if len(vector) ==0 else np.concatenate((vector, weight.flatten()))\n",
        "                elif approach == 2:\n",
        "                    weights = layer.weights\n",
        "                    vector.append(np.column_stack((weights[0].numpy().T, weights[1].numpy())))\n",
        "            vectors.append(vector)\n",
        "            labels.append([0,1] if p.find('NOISY') > 0 else [1,0])\n",
        "        else:\n",
        "            print(\"The requested model {} was not available!\".format(p + str(index)))\n",
        "    return [vectors, labels]\n",
        "\n",
        "def extract_features_deepset(processes=2, n_models=80, appr = 2, from_shadowmodel = True):\n",
        "    end = int(n_models/2)\n",
        "    global from_shadow\n",
        "    from_shadow = from_shadowmodel\n",
        "    global approach\n",
        "    approach = 2\n",
        "    if processes > 1:\n",
        "        with Pool(processes=processes) as pool:\n",
        "            train_dataset = pool.map(extract_features_from_model, range(0, end))\n",
        "    else:\n",
        "        train_dataset = []\n",
        "        for i in range(0, end):\n",
        "            train_dataset.append(extract_features_from_model(i))\n",
        "\n",
        "    print(\"Data loaded.\")\n",
        "\n",
        "    train_data_layer_1 = []\n",
        "    train_data_layer_2 = []\n",
        "    train_data_layer_3 = np.zeros(shape = (n_models,16,33))\n",
        "    train_label = []\n",
        "\n",
        "    # reconstruct the structure of weights\n",
        "    for key, t in enumerate(train_dataset):\n",
        "        for index, model in enumerate(t[0]):\n",
        "            train_data_layer_1.append(model[0])\n",
        "            train_data_layer_2.append(model[1])\n",
        "            train_data_layer_3[2*key+index] = np.asarray(model[2])\n",
        "        train_label.append(t[1])\n",
        "\n",
        "    train_data_layer_1, train_data_layer_2, train_data_layer_3 = np.asarray(train_data_layer_1), np.asarray(train_data_layer_2), np.asarray(train_data_layer_3)\n",
        "    train_label = np.asarray(train_label).reshape(n_models, 2)\n",
        "\n",
        "    return [train_data_layer_1, train_data_layer_2, train_data_layer_3], train_label\n",
        "\n",
        "def extract_features_vector(processes=1, n_models=80, appr = 0, from_shadowmodel = True):\n",
        "    end = int(n_models/2)\n",
        "    global from_shadow\n",
        "    from_shadow = from_shadowmodel\n",
        "    if from_shadow:\n",
        "        global PREFIX\n",
        "        PREFIX = 'SHADOW-'\n",
        "    global approach\n",
        "    approach = appr\n",
        "    if processes > 1:\n",
        "        with Pool(processes=processes) as pool:\n",
        "            train_dataset = pool.map(extract_features_from_model, range(0, end))\n",
        "    else:\n",
        "        train_dataset = []\n",
        "        for i in range(0, end):\n",
        "            train_dataset.append(extract_features_from_model(i))\n",
        "\n",
        "    print(\"Data loaded.\")\n",
        "\n",
        "    train_data = []\n",
        "    train_label = []\n",
        "    for train_models in train_dataset:\n",
        "        train_data.append(train_models[0])\n",
        "        train_label.append(train_models[1])\n",
        "\n",
        "    train_data = np.asarray(train_data)\n",
        "    train_label = np.asarray(train_label)\n",
        "    train_data, train_label = train_data.reshape(-1, train_data.shape[-1]), train_label.reshape(n_models, 2)\n",
        "\n",
        "    return train_data, train_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-9Ql8hJ2QkN"
      },
      "source": [
        "## Step 5 Create the meta classifier\n",
        "\n",
        "\n",
        "\n",
        "*   Baseline/Neural Sorting approach: input the feature vector, followed by three fully connected layer.\n",
        "*   Deepset approach: input weights from three layers, each layer was processed individually.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F57ZeyVI3r-7"
      },
      "source": [
        "from keras.constraints import maxnorm\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Flatten, Lambda, LocallyConnected1D, RepeatVector, Dropout\n",
        "from keras.backend import concatenate\n",
        "from tensorflow import reduce_sum\n",
        "\n",
        "def create_meta_classifier_vector():\n",
        "    input_layer = Input(shape=(105136,))\n",
        "    x = Dropout(0.3)(input_layer)\n",
        "    x = Dense(1024, activation=\"relu\")(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(16, activation=\"relu\")(x)\n",
        "    x = Dense(2, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=input_layer, outputs=x)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_meta_classifier_deepset():\n",
        "    input_1 = Input(shape=(128,785))\n",
        "    x = LocallyConnected1D(3, 1, input_shape=(128,785), activation=\"relu\")(input_1)\n",
        "    # x new output shape (128, 3)\n",
        "    output_1 = LocallyConnected1D(1, 1, activation=\"relu\")(x)\n",
        "    # x output shape (128,1) which is the vector consists of 128 node representations\n",
        "    \n",
        "\n",
        "    input_2 = Input(shape=(32,129))\n",
        "    # Flattening the output from previous layer, \n",
        "    x = Flatten()(output_1)\n",
        "    # duplicate it \n",
        "    x = RepeatVector(32)(x)\n",
        "    #  concatenate each of flattend layer ve with node from current layer\n",
        "    x = concatenate([input_2, x])\n",
        "    x = LocallyConnected1D(3, 1, input_shape=(32,257), activation=\"relu\")(x)\n",
        "    output_2 = LocallyConnected1D(1, 1, activation=\"relu\")(x)\n",
        "\n",
        "    input_3 = Input(shape=(16,33))\n",
        "    x = Flatten()(output_2)\n",
        "    x = RepeatVector(16)(x)\n",
        "    x = concatenate([input_3, x])\n",
        "    x = LocallyConnected1D(3, 1, input_shape=(16,65), activation=\"relu\")(x)\n",
        "    output_3 = LocallyConnected1D(1, 1, activation=\"relu\")(x)\n",
        "\n",
        "    x = concatenate([reduce_sum(output_1, axis=-1), reduce_sum(output_2, axis=-1), reduce_sum(output_3, axis=-1)])\n",
        "    x = Dense(4, activation=\"relu\")(x)\n",
        "    x = Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "    classifier = Model(inputs = [input_1, input_2, input_3], outputs = x)\n",
        "\n",
        "    return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqvP0yIJBouR"
      },
      "source": [
        "# Step 6 Save and Load feature representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqytxPyqBoEI"
      },
      "source": [
        "from numpy import savez_compressed, load\n",
        "\n",
        "def save_data(approach_name, n_train, n_test, train_data, test_data, train_label, test_label):\n",
        "    path = './Data/'\n",
        "    with open(path + approach_name + '{}shadow_{}target.npz'.format(n_train, n_test), 'wb') as f:\n",
        "        if not approach_name == 'deepset':\n",
        "            savez_compressed(f, train_data = train_data, train_label = train_label, test_data = test_data, test_label = test_label)\n",
        "        else:\n",
        "            savez_compressed(\n",
        "                f, \n",
        "                train_data_0 = train_data[0].copy(),\n",
        "                train_data_1 = train_data[1].copy(),\n",
        "                train_data_2 = train_data[2].copy(),\n",
        "                train_data_3 =  train_data[3].copy(),\n",
        "                train_label = train_label, \n",
        "                test_data_0 = test_data[0].copy(), \n",
        "                test_data_1 = test_data[1].copy(),\n",
        "                test_data_2 = test_data[2].copy(),\n",
        "                test_data_3 = test_data[3].copy(),\n",
        "                test_label = test_label)\n",
        "\n",
        "def load_data(approach_name, n_train, n_test):\n",
        "    path = './Data/'\n",
        "    try:\n",
        "        with load(path + approach_name + '{}shadow_{}target.npz'.format(n_train, n_test)) as loaded:\n",
        "            if not approach_name == 'deepset':\n",
        "                train_data = loaded['train_data']\n",
        "                train_label = loaded['train_label']\n",
        "                test_data = loaded['test_data']\n",
        "                test_label = loaded['test_label']\n",
        "            else:\n",
        "                train_data = [loaded['train_data_0'], loaded['train_data_1'], loaded['train_data_2'], loaded['train_data_2']]\n",
        "                train_label = loaded['train_label']\n",
        "                test_data = [loaded['test_data_0'], loaded['test_data_1'], loaded['test_data_2'], loaded['test_data_2']]\n",
        "                test_label = loaded['test_label']\n",
        "            return train_data, test_data, train_label, test_label\n",
        "    except:\n",
        "        print(\"The data file does not exist!\")\n",
        "    return\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiE2hhWx34tc"
      },
      "source": [
        "# Step 7 Train the meta classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8wuN37833xq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04336ae3-23d0-4634-c269-bbadde94d6d5"
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "file_name = input(\"Supply filename or press enter to create a new classifier:\")\n",
        "approach = int(input(\"Select the approach(0,baseline 1,sorting 2,deepset):\"))\n",
        "approach_name = ['baseline', 'neural sorting', 'deepset']\n",
        "\n",
        "load_variable = [extract_features_vector, extract_features_vector, extract_features_deepset]\n",
        "create_classifier = [create_meta_classifier_vector, create_meta_classifier_vector, create_meta_classifier_deepset]\n",
        "\n",
        "# Finished: save and load variables \n",
        "n_train = int(input(\"Supply the number of shadow models:\"))\n",
        "n_test = int(input(\"Supply the number of target models:\"))\n",
        "reload_data = input(\"Do you want to reload data?(y/n):\")\n",
        "if reload_data == 'y':\n",
        "    n_p = 1\n",
        "    train_data, train_label = load_variable[approach](n_p, n_train, approach)\n",
        "    test_data, test_label = load_variable[approach](n_p, n_test,approach, from_shadowmodel = False)\n",
        "    save_data(approach_name[approach], n_train, n_test, train_data, test_data, train_label, test_label)\n",
        "else:\n",
        "    train_data, test_data, train_label, test_label = load_data(approach_name[approach], n_train, n_test)\n",
        "    # test_data, train_data, test_label, train_label = io_variables.load_data(approach_name[approach], n_train, n_test)\n",
        "\n",
        "print(\"Start training......\")\n",
        "if file_name:\n",
        "    classifier, history = load_old_model(file_name)\n",
        "else:\n",
        "    classifier = create_classifier[approach]()\n",
        "    classifier.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    if approach == 2:\n",
        "        history = classifier.fit([train_data[0], train_data[1], train_data[2]], train_label, validation_split = 0.1, epochs=10)\n",
        "    else:\n",
        "        history = classifier.fit(train_data, train_label, validation_split = 0.1, epochs = 10)\n",
        "    history = history.history\n",
        "\n",
        "classifier.summary()\n",
        "# plot_accuracy(history)\n",
        "if approach == 2:\n",
        "    test_loss, test_accur = classifier.evaluate([test_data[0], test_data[1], test_data[2]], test_label)\n",
        "    print(\"Current approach is {}, tested on {} models, test accuracy is {}.\".format(approach_name[approach], len(test_data[0]), test_accur))\n",
        "    prediction = classifier.predict([test_data[0], test_data[1], test_data[2]])\n",
        "else:\n",
        "    test_loss, test_accur = classifier.evaluate(test_data, test_label)\n",
        "    print(\"Current approach is {}, tested on {} models, test accuracy is {}.\".format(approach_name[approach], len(test_data), test_accur))\n",
        "    prediction = classifier.predict(test_data)\n",
        "\n",
        "# plot_confusion_matrix(test_label, prediction)\n",
        "if not file_name:\n",
        "    file_name = input(\"Supply the name if you want to save the claasifier:\")\n",
        "    if file_name:\n",
        "        save_model(classifier, history, file_name)\n",
        "    else:\n",
        "        print(\"Classifier is not saved.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supply filename or press enter to create a new classifier:\n",
            "Select the approach(0,baseline 1,sorting 2,deepset):0\n",
            "Supply the number of shadow models:10\n",
            "Supply the number of target models:2\n",
            "Do you want to reload data?(y/n):y\n",
            "Data loaded.\n",
            "Data loaded.\n",
            "Start training......\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.6988 - accuracy: 0.5556 - val_loss: 0.5460 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.1094 - accuracy: 1.0000 - val_loss: 0.4106 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0450 - accuracy: 1.0000 - val_loss: 0.2751 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 3.9736e-08 - accuracy: 1.0000 - val_loss: 0.1958 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.1446 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 5.9603e-06 - accuracy: 1.0000 - val_loss: 0.1092 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0848 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0672 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0546 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0451 - val_accuracy: 1.0000\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_51 (InputLayer)        [(None, 105136)]          0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 105136)            0         \n",
            "_________________________________________________________________\n",
            "dense_200 (Dense)            (None, 1024)              107660288 \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_201 (Dense)            (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_202 (Dense)            (None, 16)                2064      \n",
            "_________________________________________________________________\n",
            "dense_203 (Dense)            (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 107,793,586\n",
            "Trainable params: 107,793,586\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Current approach is baseline, tested on 2 models, test accuracy is 1.0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG5dC0jcDR9z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}